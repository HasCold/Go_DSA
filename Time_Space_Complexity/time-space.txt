Yes, you're absolutely right! In the **modern era**, as storage and memory have become relatively cheap and abundant, optimizing for **time complexity** (speed) often takes priority over **space complexity** (memory usage). This shift allows us to focus on reducing the time it takes to compute results, even if it means using more memory to do so.

Here’s why this matters:

### 1. **Time Complexity Focus**:
   - **Faster algorithms** are crucial as data sizes grow and applications demand real-time processing. In many cases, modern systems can afford higher memory consumption if it significantly speeds up processing.
   - For example, when dealing with **big data** or high-performance computing, algorithms with low **time complexity** (like \(O(n)\) or \(O(\log n)\)) are highly preferred, even if they use more space.

### 2. **Memory Is Cheap**:
   - In most cases, the cost of memory (RAM, SSDs) has decreased dramatically, and systems can handle much larger amounts of data in memory than in the past.
   - This opens the door for **dynamic programming**, **caching** (e.g., memoization), and **precomputed tables** that use more memory to ensure faster access and calculations.

### 3. **Example – Fibonacci Calculation**:
   - In earlier times, we might have optimized for both **space** and **time** by using a recursive Fibonacci algorithm, which takes up little space but performs poorly in terms of time complexity \(O(2^n)\).
   - Now, we can use **dynamic programming** or **iterative solutions**, which use extra memory but dramatically reduce time complexity to \(O(n)\).

### 4. **Trade-Off Between Space and Time**:
   - The goal in modern algorithm design is to find a balance, but often, it’s acceptable to use more memory (like storing intermediate results) if it leads to significantly faster execution.
   - For instance, **hashing techniques** use extra memory to store keys and values, but they allow \(O(1)\) average-time lookups, which is a huge win for time-sensitive applications.

### 5. **Real-World Applications**:
   - **Databases** (like MongoDB, Redis) often use extra space to create **indexes** that speed up queries, even though these indexes take up significant memory.
   - **Web applications** use **caching layers** (e.g., Redis, Memcached) to store frequently accessed data in memory, sacrificing some space but greatly reducing time for retrieving the data.
